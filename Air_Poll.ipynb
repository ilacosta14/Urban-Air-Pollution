{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Air Pollution Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Train data\n",
    "df_train=pd.read_csv('data/Train.csv')\n",
    "\n",
    "# Upload Test data\n",
    "df_test=pd.read_csv('data/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing to drop the Place_ID X Date, as it doesn't contain any additional information\n",
    "df_train = df_train.drop('Place_ID X Date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate columns in target values, 'id' and numerical features\n",
    "target_vars = ['target', 'target_min', 'target_max', 'target_variance', 'target_count']\n",
    "id_cols = ['Place_ID', 'Date']\n",
    "num_cols = [col for col in df_train.columns if col not in target_vars + id_cols and pd.api.types.is_numeric_dtype(df_train[col])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(target_vars, axis=1)\n",
    "Y = df_train['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of nans for each column\n",
    "missing = pd.DataFrame(X_train.isnull().sum(), columns=[\"Amount\"])\n",
    "missing['Percentage'] = round((missing['Amount']/X_train.shape[0])*100, 2)\n",
    "missing[missing['Amount'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create missing data heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "missing_data = X_train.isnull()\n",
    "sns.heatmap(missing_data, yticklabels=False, cbar=True, cmap='viridis')\n",
    "plt.title('Heatmap for check of missing data\\n(Yellow = Missing, Dark = Present)', fontsize=14)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Observations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all columns with more than 50% of missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns[X_train.isna().mean() > 0.5].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = cols, axis = 1)\n",
    "X_train = X_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.drop(columns = cols, axis = 1)\n",
    "X_val = X_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here check the correlation between all features, as it appears the some have a high correlation with other features (corr >0.8). If that is the case, we drop one of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Calculates de correlations matrix\n",
    "corr_matrix = X_train.drop(['Place_ID','Date'], axis=1).corr()\n",
    "\n",
    "# Create a mask\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "corr_masked = corr_matrix.where(~mask)\n",
    "\n",
    "# Creates heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_masked.values,\n",
    "    x=corr_masked.columns,\n",
    "    y=corr_masked.columns,\n",
    "    colorscale='RdBu_r',\n",
    "    zmid=0,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    text=np.round(corr_masked.values, 3),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 8},\n",
    "    colorbar=dict(title=\"Correlation\"),\n",
    "    hovertemplate='%{y} vs %{x}<br>Correlation: %{z:.3f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Interactive Correlation Matrix of Features',\n",
    "    title_font_size=16,\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    xaxis_title='Features',\n",
    "    yaxis_title='Features',\n",
    "    xaxis={'side': 'bottom'},\n",
    "    yaxis={'autorange': 'reversed'}\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(\n",
    "    width=1400,   \n",
    "    height=1200   \n",
    ")\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features couples with corr > 0.8\n",
    "threshold = 0.8\n",
    "corr_matrix = X_train.drop(['Place_ID','Date'], axis=1).corr()\n",
    "\n",
    "# Consider only upper triangle to avoid duplicates)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# find features that have corr > threshold\n",
    "high_corr_indices = np.where((np.abs(corr_matrix) > threshold) & mask)\n",
    "\n",
    "high_corr_pairs = [\n",
    "    (corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j])\n",
    "    for i, j in zip(*high_corr_indices)\n",
    "]\n",
    "\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
    "high_corr_df = high_corr_df.sort_values('Correlation', ascending=False, key=abs)\n",
    "high_corr_df = high_corr_df.reset_index(drop=True) \n",
    "\n",
    "print(f\"Found {len(high_corr_df)} pairs with |correlation| > {threshold}\")\n",
    "print(high_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track which features have been dropped\n",
    "features_to_drop = set()\n",
    "\n",
    "# Iterate through each pair in high_corr_df\n",
    "for idx, row in high_corr_df.iterrows():\n",
    "    feature1 = row['Feature 1']\n",
    "    feature2 = row['Feature 2']\n",
    "    \n",
    "    # If feature1 is already dropped, skip this pair\n",
    "    if feature1 in features_to_drop:\n",
    "        continue\n",
    "    \n",
    "    # If feature2 is already dropped, skip this pair\n",
    "    if feature2 in features_to_drop:\n",
    "        continue\n",
    "    \n",
    "    # Drop feature2 (the second feature in the pair)\n",
    "    features_to_drop.add(feature2)\n",
    "    print(f\"Dropping {feature2} (correlated with {feature1}: {row['Correlation']:.3f})\")\n",
    "\n",
    "print(f\"\\n\\nTotal features to drop: {len(features_to_drop)}\")\n",
    "print(f\"Features: {sorted(features_to_drop)}\")\n",
    "\n",
    "# Drop from X_train and X_val\n",
    "X_train = X_train.drop(columns=features_to_drop, axis=1)\n",
    "X_val = X_val.drop(columns=features_to_drop, axis=1)\n",
    "\n",
    "print(f\"\\nNew shapes after dropping correlated features:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eda = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df_eda,corner=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between features and target (PM2.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X_train.drop(['Place_ID','Date'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.DataFrame({\n",
    "    'feature': X_num.columns,\n",
    "    'correlation': [X_num[col].corr(y_train.reset_index(drop=True)) for col in X_num.columns]\n",
    "}).sort_values('correlation', ascending=False, key=abs)\n",
    "\n",
    "print(\"Correlation of each feature with target:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "colors = ['red' if x < 0 else 'green' for x in correlations['correlation']]\n",
    "plt.barh(correlations['feature'], correlations['correlation'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Correlation with Target (PM2.5)', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Correlations with Target', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we build the steps to input missing values and apply a standard scaling on the train set and the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imputing, we consider the numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in X_train.columns if col not in id_cols and pd.api.types.is_numeric_dtype(X_train[col])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute the missing values using the mean per Place_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values per Place_ID using the group mean\n",
    "X_imputed = X_train.copy()\n",
    "# X_imputed[num_cols] = X_train.groupby('Place_ID')[num_cols].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupByPlaceIDImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, place_id='Place_ID', strategy='mean'):\n",
    "        self.place_id = place_id\n",
    "        self.strategy = strategy\n",
    "        \n",
    "    def fit(self, X, y=None):        \n",
    "        self.group_means_ = X.groupby(self.place_id)[num_cols].mean()\n",
    "        self.overall_means_ = X[num_cols].mean()\n",
    "        self.num_cols_ = num_cols\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_filled = X.copy()\n",
    "        \n",
    "        # Map Place_ID to group means\n",
    "        for col in self.num_cols_:\n",
    "            # Create a mapping: Place_ID -> mean value for this column\n",
    "            place_to_mean = self.group_means_[col].to_dict()\n",
    "            \n",
    "            # Fill NaN values\n",
    "            mask = X_filled[col].isna()\n",
    "            X_filled.loc[mask, col] = X_filled.loc[mask, self.place_id].map(place_to_mean)\n",
    "            \n",
    "            # Fill remaining NaN (for Place_IDs not in training) with overall mean\n",
    "            X_filled[col] = X_filled[col].fillna(self.overall_means_[col])\n",
    "        \n",
    "        return X_filled\n",
    "\n",
    "# Transforemer\n",
    "imputer = GroupByPlaceIDImputer(place_id='Place_ID', strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X_train)  # Transform all the Dataseet\n",
    "X_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inpute the missing values on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_imputed = imputer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the missing values again, as there could still be some extra missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.DataFrame(X_imputed.isnull().sum(), columns=[\"Amount\"])\n",
    "missing['Percentage'] = round((missing['Amount']/X_imputed.shape[0])*100, 2)\n",
    "missing[missing['Amount'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a preprocessing pipeline to impute all the missing NaNs and to scale all the data with a standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    # ('imputer', SimpleImputer(strategy='mean')),  # its optional to keep it as we already filled the missing values but its a safety layer for the future unseen data\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', pipeline, num_cols),\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column transformer changes the order of the columns, have to take this into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = num_cols + ['Date','Place_ID']\n",
    "X_preprocessed = pd.DataFrame(preprocessor.fit_transform(X_imputed), columns=column_order)\n",
    "\n",
    "for col in num_cols:\n",
    "    X_preprocessed[col] = pd.to_numeric(X_preprocessed[col], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the preprocessing pipeline on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_val_imputed), \n",
    "    columns=column_order\n",
    ")\n",
    "for col in num_cols:\n",
    "    X_val_preprocessed[col] = pd.to_numeric(X_val_preprocessed[col], errors='coerce')\n",
    "print(\"X_val_preprocessed shape:\", X_val_preprocessed.shape)\n",
    "X_val_preprocessed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good now! Let's move forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime\n",
    "X_preprocessed['Date'] = pd.to_datetime(X_preprocessed['Date'])\n",
    "X_val['Date'] = pd.to_datetime(X_val['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = X_preprocessed.drop(['Date','Place_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "reg = LinearRegression().fit(X_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = reg.predict(X_preprocessed)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_train, y_train_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Pipeline for the Random Forest and the Optimization step via GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Full Pipeline for a Random Forest Regressor\n",
    "pipe_rf = Pipeline([\n",
    "    ('rf', RandomForestRegressor(random_state=42, max_depth=10, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Perform 5-fold crossvalidation\n",
    "y_train_pred_rf = cross_val_predict(pipe_rf, X_preprocessed, y_train, cv=5)\n",
    "\n",
    "# Calculating the RSME and R²\n",
    "mse_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "\n",
    "print('Cross validation scores:')\n",
    "print('-------------------------')\n",
    "print(f'RSME: {rmse_rf}')\n",
    "print(f'R²: {r2_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original parameter settings was:\n",
    "\n",
    "*'rf__n_estimators': [50, 100, 200, 300, 500],      \n",
    "*'rf__max_depth': [5, 10, 15, 20, 30, None],          \n",
    "*'rf__min_samples_split': [2, 5, 10],          \n",
    "*'rf__min_samples_leaf': [1, 2, 4],            \n",
    "*'rf__max_features': ['sqrt', 'log2', None],  \n",
    "*'rf__bootstrap': [True, False]\n",
    "\n",
    "But the predicted time (calculaed by Claude) would have been by several hours...\n",
    "I interrupted after running 80 minutes with the below section for GridSearch (comment out) and tried RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space for Random Forest\n",
    "# Since we want to access the Random Forest step (called 'rf') in our pipeline,\n",
    "# we add 'rf__' in front of the corresponding hyperparameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_rf = {\n",
    "    'rf__n_estimators': [100, 200],       \n",
    "    'rf__max_depth': [10, 20, None],           \n",
    "    'rf__min_samples_split': [2, 10],          \n",
    "    'rf__min_samples_leaf': [1, 4],            \n",
    "    'rf__max_features': ['sqrt', None],  \n",
    "    'rf__bootstrap': [True, False]                  \n",
    "}\n",
    "\n",
    "#grid_rf = GridSearchCV(pipe_rf, param_grid=param_rf, cv=5, scoring='neg_mean_squared_error',\n",
    "                       #verbose=2, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# Fitting the model\n",
    "#grid_rf.fit(X_preprocessed, y_train)\n",
    "\n",
    "random_rf = RandomizedSearchCV(pipe_rf, param_distributions=param_rf, n_iter=20, cv=5, \n",
    "                               scoring='neg_mean_squared_error', verbose=10, n_jobs=-1, random_state=42, return_train_score=True\n",
    ")\n",
    "random_rf.fit(X_preprocessed, y_train)\n",
    "\n",
    "# Best parameters from RandomizedSearchCV\n",
    "print(\"Best parameters found: \", random_rf.best_score_)\n",
    "print(\"Best parameters found: \", random_rf.best_params_)\n",
    "\n",
    "# Save best model (including fitted preprocessing steps) as best_model \n",
    "best_rf_model = random_rf.best_estimator_\n",
    "best_rf_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
